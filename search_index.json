[["index.html", "项目分析流程 Chapter 1 Prerequisites", " 项目分析流程 Fenghu Ji 2021-05-15 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["qiime2.html", "Chapter 2 QIIME2分析流程 2.1 双端测序数据的分析流程 2.2 数据分析实例", " Chapter 2 QIIME2分析流程 QIIME2是目前分析16S最为常见的分析包，可以帮助分析者从原始DNA序列获取可供发表的图文结果。QIIME2具有一些特征: 集成和自动跟踪的数据来源 语义系统 微生物功能分析插件 多种用户接口 关于QIIME2的详细分析流程可见QIIME2的原始文档,该文档链接为2021.4月的QIIME2官方文档。 2.1 双端测序数据的分析流程 QIIME2的分析流程可见下图： 其中黄色框代表操作方法，绿色块代表文件或数据。 所有的宏基因组/扩增子测序的分析的起点是原始序列数据,原始数据多数为fastq格式，其包含有DNA序列数据和每个碱基的质量值 在样本来源未知时，需要使用barcode进行样本拆分（demultiplex），以便确定每条序列来自于哪个样本 序列去噪（denoised）以获得扩增子序列变异（amplicon sequence variants, ASVs） 在序列去噪的过程中，会由于测序数据的质量较差，导致很多样本丢失。这个时候需要使用较为严格的数据预处理，但这也会相应的减少16S序列的长度，造成下游的物种分配难点。 2.1.1 QIIME2的安装 使用conda即可轻松安装QIIME2并激活,以安装2021.4版本为例： wget https://data.qiime2.org/distro/core/qiime2-2021.4-py38-linux-conda.yml conda env create -n qiime2-2021.4 --file qiime2-2021.4-py38-linux-conda.yml # OPTIONAL CLEANUP rm qiime2-2021.4-py38-linux-conda.yml ## 激活环境 conda activate qiime2-2021.4 ## 检测安装 qiime --help 2.1.2 QIIME2的数据格式 QIIME2主要以两种输出为主，后缀分别为qza和qzv，我们分别讲述这两类文件: qza：QIIME 2 artifacts，qza文件包含文件数据和元数据（metadata），分析过程中的各类数据都被打包为qza文件格式，实际上，qza文件格式是zip文件格式的另一种说法，将文件的后缀从qza修改为zip即可以对文件进行解压操作并提取出文件内的原始内容。 qzv:visualizations。qiime2的各类可视化文件均以qzv文件格式保留，将qzv文件拖入网站QIIME2 VIEW即可查看。 2.2 数据分析实例 以文章Persistent Gut Microbiota Immaturity in Malnourished Bangladeshi Children为例，介绍分析流程。 2.2.1 数据下载 从ENA获取Accession Number为PRJEB5482的序列下载信息，数据下载完成后，在数据目录下运行命令: seqkit stat -j 40 ./* 其中，-j参数为指定运行的线程数目，该命令会统计每个序列文件的基础信息包括有序列数目，bp个数等，若某个文件下载不完整出现序列片段的缺失，则命令会对该文件进行报错。 2.2.2 导入数据 首先创建文件pe-33-manifest.txt，该文件应当包含样本的基础id和fastq序列的绝对路径，示例如下： sample-id forward-absolute-filepath reverse-absolute-filepath 1 /home/jfh/Storage3/Micro-Develop/ref_7/fastq/Bgmal10.s10_R1.fastq.gz /home/jfh/Storage3/Micro-Develop/ref_7/fastq/Bgmal10.s10_R2.fastq.gz 2 /home/jfh/Storage3/Micro-Develop/ref_7/fastq/Bgmal10.s11_R1.fastq.gz /home/jfh/Storage3/Micro-Develop/ref_7/fastq/Bgmal10.s11_R2.fastq.gz 另一个文件则为metadata文件，该文件构成样本id和文件属性的对应: sample-id sample-name group #q2:types categorical categorical 1 Bgmal10.s10 course_of_the_SAM_trial 2 Bgmal10.s11 course_of_the_SAM_trial 3 Bgmal10.s12 course_of_the_SAM_trial 4 Bgmal10.s13 course_of_the_SAM_trial 5 Bgmal10.s1 course_of_the_SAM_trial 6 Bgmal10.s3 course_of_the_SAM_trial 将数据导入到qza文件中 qiime tools import --type &#39;SampleData[PairedEndSequencesWithQuality]&#39; \\ --input-path pe-33-manifest.txt --output-path pe_sequence.qza \\ --input-format PairedEndFastqManifestPhred33V2 ## 获得可视化的质量图形 qiime demux summarize --i-data pe_sequence.qza \\ --o-visualization pe_sequence.qzv 将生成的pe_sequence.qzv导入到qiime2view中进行查看。可以看到如下结果： 从图中可以看到，reverse数据的质量很差，在140bp时就出现了数据质量的大幅度下降。 下面为数据基本统计： Minimum: 2545 Median: 21910.0 Mean: 31070.940995382247 Maximum: 887651 Total: 60557264 数据质控 dada2去噪会由于样本测序质量较差而丢失样本，在本研究中尤甚，故重新选择了deblur去噪，但该去噪方案速度极慢,但是需要注意的是，在大部分的分析中，以DADA2去噪为主流方案。关于DADA2的去噪方法请查看官网moving tutorial教程 qiime quality-filter q-score \\ --i-demux pe_sequence.qza \\ --o-filtered-sequences demux-filtered.qza \\ --o-filter-stats demux-filter-stats.qza ## 只保留120bp的片段 qiime deblur denoise-16S \\ --i-demultiplexed-seqs demux-filtered.qza \\ --p-trim-length 120 \\ --o-representative-sequences rep-seqs-deblur.qza \\ --o-table table-deblur.qza \\ --p-sample-stats \\ --o-stats deblur-stats.qza #生成ASV table qiime feature-table summarize --i-table table-deblur.qza --o-visualization table-deblur.qzv mv table-deblur.qza table.qza ;mv rep-seqs-deblur.qza rep-seqs.qza 多序列比对和构建进化树 #可视化feature qiime feature-table summarize --i-table table.qza --o-visualization table.qzv ## 构建进化树 qiime phylogeny align-to-tree-mafft-fasttree \\ --i-sequences rep-seqs.qza \\ --o-alignment aligned-rep-seqs.qza \\ --o-masked-alignment masked-aligned-rep-seqs.qza \\ --o-tree unrooted-tree.qza \\ --o-rooted-tree rooted-tree.qza 从上述文件Healthy-filtered-table.qzv观测样本的特征，Feature Count最小为2324，为了保证复现，我们在稀释的时候应当保证每一个样本都能够保留。故按照 2324的标准进行稀释。 查看稀释曲线 qiime diversity alpha-rarefaction \\ --i-table table.qza \\ --i-phylogeny rooted-tree.qza \\ --p-max-depth 2324 \\ --m-metadata-file metadata.txt \\ --o-visualization alpha-rarefaction.qzv 将文件alpha-rarefaction.qzv在QIIME2 VIEW中查看，大部分样本在2324时ASV数目基本趋于饱和。 稀释样本 qiime diversity core-metrics-phylogenetic \\ --i-phylogeny rooted-tree.qza \\ --i-table Healthy-filtered-table.qza \\ --p-sampling-depth 2324 \\ --m-metadata-file metadata.txt \\ --output-dir core-metrics 物种分类 qiime feature-classifier classify-sklearn \\ --i-classifier gg-13-8-99-nb-classifier.qza \\ --i-reads rep-seqs.qza \\ --o-classification taxonomy.qza qiime metadata tabulate \\ --m-input-file taxonomy.qza \\ --o-visualization taxonomy.qzv qiime taxa barplot \\ --i-table core-metrics/rarefied_table.qza \\ --i-taxonomy taxonomy.qza \\ --m-metadata-file metadata.txt \\ --o-visualization rarefied_taxa-bar-plots.qzv 不同水平的物种丰度结果可以直接从rarefied_taxa-bar-plots.qzv中获取，而每一个ASV的丰度则位于文件rarefied_table.qza中，我们可以将该文件解压，获取其中data目录下的biom文件 mkdir -p ASV; cd ASV cp ../core-metrics/rarefied_table.qza rarefied_table.zip unzip rarefied_table.zip; # 需要对解压下的文件夹重命名，比如命名为rarefied_table cd rarefied_table/data; biom convert -i feature-table.biom -o asv_table.txt --to-tsv 这样便可以获取每一个ASV的丰度了。 "],["RandomForest.html", "Chapter 3 随机森林模型 3.1 随机森林原理 3.2 随机森林变量importance 3.3 幼儿微生物随机森林回归 3.4 数据预览 3.5 分析代码解析", " Chapter 3 随机森林模型 3.1 随机森林原理 随机森林是集成学习中的Bagging（Bootstrap AGgregation）方法，其由多个决策树组成，当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。关于随机森林的原理，可以参看博文等。 随机森林模型可以用于分类任务和回归任务中，下图展示了随机森林的基本回归原理： 随机森林回归的预测结果是由内部所有二叉决策树的预测结果取平均值得到的。二叉决策树的预测过程主要分为以下步骤： 针对某一输入样本，从二叉决策树的根节点起，判断当前节点是否为叶子节点，如果是则返回叶子节点的预测值(即当前叶子中样本目标变量的平均值），如果不是则进入下一步 根据当前节点的切分变量的和切分值，将样本中对应变量的值与节点的切分值对比。如果样本变量值小于等于当前节点切分值，则访问当前节点的左子节点；如果样本变量值大于当前节点切分值，则访问当前节点的右子节点 循环步骤2，直到访问到叶子节点，并返回叶子节点的预测 3.2 随机森林变量importance 在随机森林模型中，每一个变量都有自己的importance score，这个importance score是该变量对于决策树预测的力度来决定的，在一个模型中，单个变量在树的最上侧，则该变量对于预测的影响程度越大，一般这种变量会大大减少预测结果的熵，对单个样本的预测具有十分重要的作用。 在大部分的随机森林模型中，总是会先对变量进行重要性评估，然后再挑选出部分最重要的变量来作为最终的模型预测结果。这种思路我们会在之后的分析中展示。 3.3 幼儿微生物随机森林回归 关于随机森林的源代码在GitHub上。 我们的假设是，幼儿的肠道微生物组成能够代表幼儿的年龄。多种微生物作为变量，可以对最终的年龄进行回归。 我们首先将ASV的丰度进行标准化，转换为相对丰度。文件放置于asv_realtive_abundance.txt。其次是样本的metadata和每一个ASV对应的物种分类信息. 3.4 数据预览 原文：Persistent Gut Microbiota Immaturity in Malnourished Bangladeshi Children 数据从ENA中进行下载，数据号PRJEB5482,下载数据共2811个样本，其中部分数据采取不同的run测序，根据样本id将数据合并，一共得到1949个样本。 其中健康婴儿的样本数目共996个，包含有Healthy Singleton Birth Cohort,Healthy Twins &amp; Triplets两类。在本文的分析中，首先使用12个孩子(包含272个16S样本)来训练随机森林模型，然后再分别在13个Singleton 孩子(276个16S样本)和25个Twins &amp; Triplets孩子（447）个样本中进行随机森林模型的验证计算。 基本统计见下表： Accession Numer：PRJEB5482 数据类型 数据用途 Child Number Sample Number Country：Bangladesh Singleton Training 12 272 Child Type:Healthy Singleton Validation 13 276 Total Samples:996 Twins &amp; Triplets Validation 25 448 3.5 分析代码解析 rm(list=ls()) library(randomForest) library(dplyr) #读入ASV丰度 asv_abundance &lt;- read.table(&quot;asv_realtive_abundance.txt&quot;,header=T,sep=&quot;\\t&quot;) ## 过滤ASV ## 创建函数来计算符合相对丰度&gt;0.1%的ASV个数 count &lt;- function(abun){ num &lt;- sum(abun&gt;0.1) return(num) } filter_count &lt;- apply(asv_abundance,1,count) index &lt;- which(filter_count&gt;2) # 获取符合条件的ASV索引 filter_abundance &lt;- asv_abundance[index,] ## 读入metadata数据并合并数据 metadata &lt;- read.table(&quot;healthy_metadata.txt&quot;,header=T,sep=&quot;\\t&quot;,row.names=1) otu &lt;- data.frame(t(filter_abundance)) otu$age_day &lt;- metadata[rownames(otu),]$age_day # 训练集和测试集 train_sample &lt;- otu[rownames(metadata)[which(metadata$Type==&quot;Training&quot;)],] valid_sample &lt;- otu[rownames(metadata)[which(metadata$Type==&quot;Validation&quot;)],] set.seed(123) # 创建随机种子，从而使得结果可重复 otu_train.forest &lt;- randomForest(age_day~., data = train_sample, importance = TRUE) ## 开始训练 当前的模型是以所有的ASV作为预测变量，我们查看一下当前的性能。 otu_train.forest ## ## Call: ## randomForest(formula = age_day ~ ., data = train_sample, importance = TRUE) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 201 ## ## Mean of squared residuals: 16052.08 ## % Var explained: 64.26 使用605个ASVs作为特征输入，对文中的chronologic age进行回归，chronologic age使用的数据为婴儿的出生天数。 结果显示，使用所有的ASV作为输入的时候能够解释64.26%的variance，而原文为73%。考虑到实际只有部分ASV可以作为变量解释，过多的变量会影响模型，我们从上述训练模型获取每个变量的importance score. 3.5.1 查看importance score importance_otu &lt;- data.frame(otu_train.forest$importance) # 下面的绘图代码结果见下图 #varImpPlot(otu_train.forest, n.var = min(30, nrow(otu_train.forest$importance)), #main = &#39;Top 30 - variable importance&#39;) 3.5.2 获取最佳变量个数 这里有两个变量： %IncMSE：increase in mean squared error，通过对每一个预测变量随机赋值，如果该预测变量更为重要，那么其值被随机替换后模型预测的误差会增大。因此，该值越大表示该变量的重要性越大 IncNodePurity：increase in node purity，通过残差平方和来度量，代表了每个变量对分类树每个节点上观测值的异质性的影响，从而比较变量的重要性。该值越大表示该变量的重要性越大。 我们使用IncNodePurity来衡量importance score，并通过十折交叉验证筛选变量. importance_otu &lt;- importance_otu[order(importance_otu$IncNodePurity, decreasing = TRUE), ] #write.table(importance_otu, &#39;importance_otu.txt&#39;, sep = &#39;\\t&#39;, col.names = NA, quote = FALSE) 3.5.3 查看不同变量个数下的模型误差 otu_train &lt;- train_sample set.seed(123) otu_train.cv &lt;- replicate(10, rfcv(otu_train[-ncol(otu_train)], otu_train$age_day, cv.fold = 10, step = 1.5), simplify = FALSE) otu_train.cv otu_train.cv &lt;- data.frame(sapply(otu_train.cv, &#39;[[&#39;, &#39;error.cv&#39;)) otu_train.cv$otus &lt;- rownames(otu_train.cv) otu_train.cv &lt;- reshape2::melt(otu_train.cv, id = &#39;otus&#39;) otu_train.cv$otus &lt;- as.numeric(as.character(otu_train.cv$otus)) otu_train.cv.mean &lt;- aggregate(otu_train.cv$value, by = list(otu_train.cv$otus), FUN = mean) 3.5.4 查看结果 #head(otu_train.cv.mean, 10) #拟合线图 library(ggplot2) ggplot(otu_train.cv.mean, aes(Group.1, x)) + geom_line() + theme(panel.grid = element_blank(), panel.background = element_rect(color = &#39;black&#39;, fill = &#39;transparent&#39;)) + labs(title = &#39;&#39;,x = &#39;Number of OTUs&#39;, y = &#39;Cross-validation error&#39;) 十折交叉验证的结果表明，当变量数在16-24时，模型误差开始减慢减少速度，在24-35的时候基本减少的误差很小甚至开始增加。所以我们可以认为，该模型筛选的变量数目和原文比较一致，在ASV变量数目为24时，达到了最好的回归效果。所以只需要保留Top24 importance score的ASVs作为预测变量即可。 3.5.5 24个变量的简约回归 importance_otu.select &lt;- importance_otu[1:24, ] otu_id.select &lt;- rownames(importance_otu.select) #write.table(importance_otu.select, &#39;importance_otu.select.txt&#39;, sep = &#39;\\t&#39;, col.names = NA, quote = FALSE) ##只包含 24 个重要预测变量的简约回归 otu_train.select &lt;- train_sample[,c(otu_id.select,&quot;age_day&quot;)] otu_test.select &lt;- valid_sample[,c(otu_id.select,&quot;age_day&quot;)] #随机森林计算（默认生成 500 棵决策树），详情 ?randomForest set.seed(123) otu_train.select.forest &lt;- randomForest(age_day~., data = otu_train.select, importance = TRUE) 查看这24个变量的回归性能： otu_train.select.forest ## ## Call: ## randomForest(formula = age_day ~ ., data = otu_train.select, importance = TRUE) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 8 ## ## Mean of squared residuals: 15775.65 ## % Var explained: 64.88 上述结果表明，当预测变量数目为24时，达到了比所有ASVs作为预测变量更好的变量解释程度：64.88 接下来我们使用测试集数目进行评分，测试集数目分为Singleton和Twins &amp; Triplets数据. 3.5.6 Singleton预测结果 names &lt;- rownames(otu_test.select) single_test &lt;- otu_test.select[which(grepl(names,pattern=&quot;Bgs&quot;)==TRUE),] twin_test &lt;- otu_test.select[which(grepl(names,pattern=&quot;Bgt&quot;)==TRUE),] age_predict_single &lt;- predict(otu_train.select.forest, single_test) plot(single_test$age_day, age_predict_single, main = &#39;Single_Birth_Cohort_Test&#39;,xlab = &#39;age (days)&#39;, ylab = &#39;Predict&#39;) abline(1, 1) Single_data &lt;- data.frame(single_test$age_day, age_predict_single) Single_Fit &lt;- lm(age_predict_single~single_test.age_day,data=Single_data) summary(Single_Fit) ## ## Call: ## lm(formula = age_predict_single ~ single_test.age_day, data = Single_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -340.61 -66.55 -1.79 64.17 308.49 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 115.47437 11.21453 10.30 &lt;2e-16 *** ## single_test.age_day 0.71214 0.02805 25.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 97.4 on 274 degrees of freedom ## Multiple R-squared: 0.7017, Adjusted R-squared: 0.7006 ## F-statistic: 644.5 on 1 and 274 DF, p-value: &lt; 2.2e-16 3.5.7 Twins &amp; Triplets预测结果 age_predict_twin &lt;- predict(otu_train.select.forest, twin_test) plot(twin_test$age_day, age_predict_twin, main = &#39;Twin_Birth_Cohort_Test&#39;,xlab = &#39;age (days)&#39;, ylab = &#39;Predict&#39;) abline(1, 1) Twin_data &lt;- data.frame(twin_test$age_day, age_predict_twin) Twin_Fit &lt;- lm(age_predict_twin~twin_test.age_day,data=Twin_data) summary(Twin_Fit) ## ## Call: ## lm(formula = age_predict_twin ~ twin_test.age_day, data = Twin_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -326.82 -69.89 -7.75 63.92 277.00 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 138.48008 8.50261 16.29 &lt;2e-16 *** ## twin_test.age_day 0.74909 0.02631 28.47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 100.3 on 446 degrees of freedom ## Multiple R-squared: 0.645, Adjusted R-squared: 0.6442 ## F-statistic: 810.4 on 1 and 446 DF, p-value: &lt; 2.2e-16 上述结果表明： 在Singleton和Twins &amp; Triplets的预测结果和原始的结果拟合，拟合R^2分别为0.7006和0.6442 Taxa Number 变量解释 测试集Singleton拟合R^2 测试集多胞胎拟合R^2 原文 24 unknown 0.71 0.68 复现 24 64.88 0.7006 0.6442 看一下这个OTU是什么： # write.table(importance_otu.select,&quot;Selected_OTUs.txt&quot;,sep=&quot;\\t&quot;,quote=F) # 需要对importance_otu.select表格中的&quot;X&quot;替换掉 taxa &lt;- read.table(&quot;taxonomy.txt&quot;,sep=&quot;\\t&quot;,header=T) ASV &lt;- read.table(&quot;Selected_OTUs.txt&quot;,header=T,sep=&quot;\\t&quot;) ASV_infomation &lt;- inner_join(taxa,ASV,by=&quot;ASV&quot;) show_info &lt;- ASV_infomation[,c(1,2)] show_info[,2] "]]
